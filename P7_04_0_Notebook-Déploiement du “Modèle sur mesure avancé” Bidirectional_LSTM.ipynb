{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.26.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "#import mlflow\n",
    "#import mlflow.azureml\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WS-JCROSELLO-P7\twesteurope\tcloud-shell-storage-westeurope\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config(path=\"config.json\")\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/75c51acd-24dc-4674-9a83-dd6319951bf8/resourceGroups/cloud-shell-storage-westeurope/providers/Microsoft.MachineLearningServices/workspaces/WS-JCROSELLO-P7',\n",
       " 'name': 'WS-JCROSELLO-P7',\n",
       " 'identity': {'principal_id': '9cc66bc7-ee73-42a6-b9af-e123e8e136d0',\n",
       "  'tenant_id': '31f99866-f9ca-40e6-bc44-16e162aed2f1',\n",
       "  'type': 'SystemAssigned'},\n",
       " 'location': 'westeurope',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'tags': {},\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': 'ae81e516-8e98-48db-8aa2-005fbca07768',\n",
       " 'sdkTelemetryAppInsightsKey': '6d7b2b55-6d18-4126-a3ee-251f78cfa6e5',\n",
       " 'description': '',\n",
       " 'friendlyName': 'WS-JCROSELLO-P7',\n",
       " 'creationTime': '2021-04-14T08:30:38.6122892+00:00',\n",
       " 'containerRegistry': '/subscriptions/75c51acd-24dc-4674-9a83-dd6319951bf8/resourceGroups/cloud-shell-storage-westeurope/providers/Microsoft.ContainerRegistry/registries/ae81e5168e9848db8aa2005fbca07768',\n",
       " 'keyVault': '/subscriptions/75c51acd-24dc-4674-9a83-dd6319951bf8/resourcegroups/cloud-shell-storage-westeurope/providers/microsoft.keyvault/vaults/wsjcrosellop78312448993',\n",
       " 'applicationInsights': '/subscriptions/75c51acd-24dc-4674-9a83-dd6319951bf8/resourcegroups/cloud-shell-storage-westeurope/providers/microsoft.insights/components/wsjcrosellop76525879486',\n",
       " 'storageAccount': '/subscriptions/75c51acd-24dc-4674-9a83-dd6319951bf8/resourcegroups/cloud-shell-storage-westeurope/providers/microsoft.storage/storageaccounts/wsjcrosellop79414096984',\n",
       " 'hbiWorkspace': False,\n",
       " 'allowPublicAccessWhenBehindVnet': False,\n",
       " 'provisioningState': 'Succeeded',\n",
       " 'imageBuildCompute': '',\n",
       " 'discoveryUrl': 'https://westeurope.experiments.azureml.net/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-ws-jcrosello-westeurope-ae81e516-8e98-48db-8aa2-005fbca07768.notebooks.azure.net',\n",
       "  'resource_id': 'fb4bdc297b984d3eb5332e045e013a32'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see workspace details\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "import os\n",
    "\n",
    "\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn import decomposition\n",
    "from sklearn import preprocessing\n",
    "sns.set()\n",
    "# Python program to read \n",
    "# json file \n",
    " \n",
    "import json \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.optimizers import TFOptimizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stemming = PorterStemmer()\n",
    "Lemmatizer = WordNetLemmatizer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops2 = set(stopwords.words(\"french\"))\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# The following import and function call are the only additions to code required\n",
    "# to automatically log metrics and parameters to MLflow.\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Prepare the dataset...\")\n",
    "# Prepare the dataset\n",
    "commentData = pd.read_csv('P7_01_0_training.1600.processed.noemoticon.csv', encoding='ISO-8859-1')\n",
    "df = commentData\n",
    "train_df = df.drop([\"id\",\"date\",\"flag\",\"flag\",\"user\"], axis=1)\n",
    "train_df = train_df.rename(columns={\"target\": \"Sentiment\"})\n",
    "\n",
    "def clean_str(x):\n",
    "    \n",
    "    x = str(x)\n",
    "   \n",
    "    # Convert to lower case\n",
    "    text = x.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Keep only words (removes punctuation + numbers)\n",
    "    # use .isalnum to keep also numbers\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    \n",
    "    # Stemming\n",
    "    #stemmed_words = [stemming.stem(w) for w in token_words]\n",
    "\n",
    "    # lemmatizer    \n",
    "    lemmatizer_words = [Lemmatizer.lemmatize(w) for w in token_words]\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words = [w for w in token_words if not w in stops]\n",
    "    \n",
    "    # Remove stop words\n",
    "    meaningful_words2 = [w for w in meaningful_words if not w in stops2]   \n",
    "    \n",
    "    # Rejoin meaningful stemmed words\n",
    "    joined_words = ( \" \".join(meaningful_words2))\n",
    "\n",
    "    \n",
    "    # Return cleaned data\n",
    "    return joined_words\n",
    "\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_str)\n",
    "\n",
    "df = train_df\n",
    "df_0 = df[df['Sentiment'] == 0].sample(frac=1)\n",
    "df_4 = df[df['Sentiment'] == 4].sample(frac=1)\n",
    "\n",
    "# we want a balanced set for training against - there are 7072 `0` examples\n",
    "sample_size = min(len(df_0), len(df_4))\n",
    "\n",
    "data = pd.concat([df_0.head(sample_size), df_4.head(sample_size)]).sample(frac=1)\n",
    "sentences = data['text']\n",
    "tokenizer = Tokenizer(num_words = 4000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "embed_num_dims = 100\n",
    "max_seq_len = 1000\n",
    "padded_seq = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = pd.get_dummies(data['Sentiment']).values\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(padded_seq,Y ,train_size = 0.55)\n",
    "\n",
    "# Glove\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "embedd_index = {}\n",
    "for line in f:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    coff = np.asarray(val[1:],dtype = 'float')\n",
    "    embedd_index[word] = coff\n",
    "\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((len(index_of_words) + 1, embed_num_dims))\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "\n",
    "for word,i in index_of_words.items():\n",
    "    temp = embedd_index.get(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp\n",
    "        \n",
    "#for plotting\n",
    "        tokens.append(embedding_matrix[i])\n",
    "        labels.append(word)\n",
    "\n",
    "#Embedding layer before the actaul BLSTM \n",
    "embedd_layer = Embedding(len(index_of_words) + 1 , embed_num_dims , input_length = max_seq_len , weights = [embedding_matrix])\n",
    "\n",
    "print(\"Building model...\")\n",
    "# Bidirectional LSTM model avec GLOVE \n",
    "model = Sequential()\n",
    "model.add(embedd_layer)\n",
    "model.add(Bidirectional(LSTM(30 , return_sequences = True , dropout = 0.1 , recurrent_dropout = 0.1)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(30,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation = 'sigmoid'))\n",
    "\n",
    "# Train LSTM model\n",
    "add = Adam(lr = 0.01)\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = add , metrics = ['accuracy'])\n",
    "hist = model.fit(X_train,Y_train,epochs = 5 , batch_size = 100, validation_data = (X_test,Y_test))\n",
    "\n",
    "score = model.evaluate(X_test,Y_test, batch_size= 100, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "print(\"Accuracy: %.2f%%\" % (score[1] * 100))\n",
    "# Save the model \n",
    "#for heavy model architectures, .h5 file is unsupported.\n",
    "weigh= model.get_weights();    pklfile= \"./model1.pkl\"\n",
    "try:\n",
    "    fpkl= open(pklfile, 'wb')    #Python 3     \n",
    "    pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "    fpkl.close()\n",
    "except:\n",
    "    fpkl= open(pklfile, 'w')    #Python 2      \n",
    "    pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "    fpkl.close()\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.optimizers import TFOptimizer\n",
    "import tensorflow as tf\n",
    "model_path = \"./model\"\n",
    "tf.keras.models.save_model(model, model_path, overwrite=True, include_optimizer=True, save_format=None)\n",
    "\n",
    "import onnxmltools\n",
    "\n",
    "onnx_model = onnxmltools.convert_keras(model) \n",
    "\n",
    "onnxmltools.utils.save_model(onnx_model, 'model.onnx')\n",
    "\n",
    "# Save the model \n",
    "from tensorflow.python.keras.optimizers import TFOptimizer\n",
    "import tensorflow as tf\n",
    "model_path = \"./model.h5\"\n",
    "tf.keras.models.save_model(model, model_path, overwrite=True, include_optimizer=True, save_format='h5')\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jcrro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jcrro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcrro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare the dataset...\n",
      "Building model...\n",
      "WARNING:tensorflow:From C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\jcrro\\anaconda3\\envs\\EnvP7\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 880 samples, validate on 720 samples\n",
      "Epoch 1/5\n",
      "880/880 [==============================] - 11s 13ms/step - loss: 0.6754 - accuracy: 0.5841 - val_loss: 0.6504 - val_accuracy: 0.6333\n",
      "Epoch 2/5\n",
      "880/880 [==============================] - 11s 13ms/step - loss: 0.5040 - accuracy: 0.7625 - val_loss: 0.6112 - val_accuracy: 0.7014\n",
      "Epoch 3/5\n",
      "880/880 [==============================] - 11s 13ms/step - loss: 0.2820 - accuracy: 0.8886 - val_loss: 0.8649 - val_accuracy: 0.6639\n",
      "Epoch 4/5\n",
      "880/880 [==============================] - 11s 13ms/step - loss: 0.1142 - accuracy: 0.9568 - val_loss: 0.9297 - val_accuracy: 0.6833\n",
      "Epoch 5/5\n",
      "880/880 [==============================] - 12s 13ms/step - loss: 0.0453 - accuracy: 0.9830 - val_loss: 1.1685 - val_accuracy: 0.6958\n",
      "720/720 [==============================] - 2s 3ms/step\n",
      "Test score: 1.168547261092398\n",
      "Test accuracy: 0.6958333253860474\n",
      "Accuracy: 69.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX operator number change on the optimization: 22 -> 16\n"
     ]
    }
   ],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model my_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from azureml.core.model import Model\n",
    "# Tip: When model_path is set to a directory, you can use the child_paths parameter to include\n",
    "#      only some of the files from the directory\n",
    "model = Model.register(model_path = \"model.h5\",\n",
    "                       model_name = \"my_model\",\n",
    "                       description = \"P7 Analyse sentiments1-Déploiement du Modèle sur mesure avancé\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model(ws, 'my_model', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='WS-JCROSELLO-P7', subscription_id='75c51acd-24dc-4674-9a83-dd6319951bf8', resource_group='cloud-shell-storage-westeurope'), name=my_model, id=my_model:1, version=1, tags={}, properties={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from azureml.core.model import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_seq_len = 1000\n",
    "\n",
    "# Called when the deployed service starts\n",
    "def init():\n",
    "    global model\n",
    "\n",
    "    \n",
    " \n",
    "    # load models\n",
    "    #model = Model.get_model_path(model_name = 'my_model')\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), './model.h5')\n",
    "    model = model = load_model(model_path)    \n",
    "    \n",
    "\n",
    "# Handle requests to the service\n",
    "def run(data):\n",
    "    try:\n",
    "        data = json.loads(data)    \n",
    "        tokenizer = Tokenizer(num_words = 100)   \n",
    "        _seq = tokenizer.texts_to_sequences(data['text'])\n",
    "        x_test = pad_sequences(_seq, maxlen=max_seq_len)\n",
    "        result = np.mean(np.array([[float(x)] for x in model.predict([x_test])[0]]))\n",
    "        # you can return any datatype as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create the environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "# Define the packages needed by the model and scripts\n",
    "conda_dep.add_conda_package(\"tensorflow\")\n",
    "conda_dep.add_conda_package(\"numpy\")\n",
    "conda_dep.add_conda_package(\"scikit-learn\")\n",
    "conda_dep.add_conda_package(\"pandas\")\n",
    "\n",
    "# You must list azureml-defaults as a pip dependency\n",
    "conda_dep.add_pip_package(\"azureml-defaults\")\n",
    "conda_dep.add_pip_package(\"keras\")\n",
    "conda_dep.add_pip_package(\"tensorflow\")\n",
    "conda_dep.add_pip_package(\"nltk\")\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(conda_dep.serialize_to_string())\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "myenv.python.conda_dependencies=conda_dep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-04-16 10:59:40+02:00 Creating Container Registry if not exists.\n",
      "2021-04-16 10:59:41+02:00 Registering the environment.\n",
      "2021-04-16 10:59:42+02:00 Use the existing image.\n",
      "2021-04-16 10:59:43+02:00 Generating deployment configuration.\n",
      "2021-04-16 10:59:44+02:00 Submitting deployment to compute..\n",
      "2021-04-16 10:59:51+02:00 Checking the status of deployment myservice..\n",
      "2021-04-16 11:05:02+02:00 Checking the status of inference endpoint myservice.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "# Register the model to deploy\n",
    "\n",
    "\n",
    "\n",
    "# Combine scoring script & environment in Inference configuration\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "# Set deployment configuration\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 4)\n",
    "\n",
    "# Define the model, inference, & deployment configuration and web service name and location to deploy\n",
    "service = Model.deploy(\n",
    "    workspace = ws,\n",
    "    name = \"myservice\",\n",
    "    models = [model],\n",
    "    inference_config = inference_config,\n",
    "    deployment_config = deployment_config)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test du modèle web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0.33497992157936096\n",
      "0.33497992157936096\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "scoring_uri = 'http://0adf4c9f-4bbd-4167-9a23-dca73c35ba35.westeurope.azurecontainer.io/score'\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "\n",
    "test_data = json.dumps({'text': 'today is a great day'})\n",
    "\n",
    "response = requests.post(scoring_uri, test_data, headers=headers)\n",
    "print(response.status_code)\n",
    "\n",
    "print(response.json())\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
